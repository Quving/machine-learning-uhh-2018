from sklearn.datasets import fetch_20newsgroups
import numpy as np

'''
20 newsgroup dataset

access...
... filenames:      twenty_train.filenames
... data:           twenty_train.data  
... target:         twenty_train.target        # category integer ID
... target names:   twenty_train.target_names  # category names
'''

categories=[
    'alt.atheism', 
    'comp.graphics', 
    'sci.med', 
    'soc.religion.christian'
]

twenty_train = fetch_20newsgroups(
    categories=categories,
    shuffle=True, 
    random_state=41 
)

# A: How many texts have been loaded?

print('''
target:     {}
# filenames: {}
# data:      {}
# target:    {}
'''.format(
    twenty_train.target_names,
    len(twenty_train.filenames),
    len(twenty_train.data),
    len(twenty_train.target)
))

print('''
Q: How many texts have been loaded?
A: {} texts have been downloaded.
'''.format(
    len(twenty_train.filenames)
))

# B

# Follow the tutorial and use sklearn.feature extraction.text.CountVectorizer to tokenize and generate the list of words from the training data.

words = []
dict_words = {}

# Assign a fixed integer id to each word occuring in any document of the training set
from sklearn.feature_extraction.text import CountVectorizer

corpus = twenty_train.data
vectorizer = CountVectorizer()

# for each document, count the number of occurences of each word w and store it as tuple as (i, j) (document id, word id)
X = vectorizer.fit_transform(corpus)

# How many words were found?


# How do you access the list of words?
# How do you find the numerical index of a word in the feature vectors?

# C

# Continue the tutorial and train the MultinomialNB Bayes classifier using the feature vectors generated by CountVectorizer. 
# Test and classify a few texts (strings) of your own. 
# Then finish the tutorial with building the Pipeline and classify the texts from the test set.

# D

# What is the motivation for the stop words argument of CountVectorizer? 
# Try running the full example (including tokening, training the classifier, 
# and running classification on the test set) again when using stop worlds=’english’. 
# How many words are found? 
# What is the performance (accuracy)?

# Read the paragraph about term frequencies and normalization using TfidfTransformer or TfidfVectorizer. 
# What is the motivation for working with frequencies instead of word counts?