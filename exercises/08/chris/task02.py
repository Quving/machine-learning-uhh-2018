from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
import numpy as np

def get_data():
    '''
    20 newsgroup dataset

    access...
    ... filenames:      twenty_train.filenames
    ... data:           twenty_train.data  
    ... target:         twenty_train.target        # category integer ID
    ... target names:   twenty_train.target_names  # category names
    '''

    categories=[
        'alt.atheism', 
        'comp.graphics', 
        'sci.med', 
        'soc.religion.christian'
    ]

    twenty_train = fetch_20newsgroups(
        categories=categories,
        shuffle=True, 
        random_state=41 
    )

    # A: How many texts have been loaded?

    print('''
    target:     {}
    # filenames: {}
    # data:      {}
    # target:    {}
    '''.format(
        twenty_train.target_names,
        len(twenty_train.filenames),
        len(twenty_train.data),
        len(twenty_train.target)
    ))

    print('''
    Q: How many texts have been loaded?
    A: {} texts have been downloaded.
    '''.format(
        len(twenty_train.filenames)
    ))

    words = []
    dict_words = {}

    # Assign a fixed integer id to each word occuring in any document of the training set

    corpus = twenty_train.data
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus)
    y = twenty_train.target
    label_names = twenty_train.target_names

    # for each document, count the number of occurences of each word w and store it as tuple as (i, j) (document id, word id)

    print('Q: How many words were found?')
    print('A: There are {} different words in the documents.'.format(len(vectorizer.get_feature_names())))
    print('\n')
    print('Q: How do you access the list of words?')
    print('A: By calling get_feature_names().')
    print('\n')
    print('Q: How do you find the numerical index of a word in the feature vectors?')
    print('A: By calling .vocabulary_.get("word")')
    print('\n')

    return X, y, label_names

# B

# Follow the tutorial and use sklearn.feature extraction.text.CountVectorizer to tokenize and generate the list of words from the training data.

def train_classifier(X, y):


    # C

    # Continue the tutorial and train the MultinomialNB Bayes classifier using the feature vectors generated by CountVectorizer. 
    classifier = MultinomialNB()
    classifier.fit(X, y)

    return classifier

def predict(predict_content, classifier):
    result = classifier.predict(predict_content)
    proba = classifier.predict_proba(predict_content)
    score = classifier.score(predict_content, )

    return result, proba, score

if __name__ == "__main__":

    X, y, label_names = get_data()

    classifier = train_classifier(X, y)

    prediction, proba = predict(X[2:3], classifier)

    prediction = label_names[prediction[0]]

    print('Prediction: {}, Probability: {}'.format(prediction, proba))

# Test and classify a few texts (strings) of your own. 
# Then finish the tutorial with building the Pipeline and classify the texts from the test set.

# D

# What is the motivation for the stop words argument of CountVectorizer? 
# Try running the full example (including tokening, training the classifier, 
# and running classification on the test set) again when using stop worlds=’english’. 
# How many words are found? 
# What is the performance (accuracy)?

# Read the paragraph about term frequencies and normalization using TfidfTransformer or TfidfVectorizer. 
# What is the motivation for working with frequencies instead of word counts?